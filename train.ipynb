{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d13925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from model import build_transformer\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from dataset import BilingualDataset,casual_mask\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from config import get_config,get_weights_file_path\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc0c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating the tokenizer\n",
    "\n",
    "def get_all_sentences(ds,lang):\n",
    "    '''This function takes in the dataset and lang and what it does is iterate the dataset and returns all sentences in one particular language'''\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def build_tokenizer(config,ds,lang):\n",
    "    '''This functions builds and saves the tokenizer if it does not exist, if it exist it just fetches the tokenizer. It returns the tokenizer'''\n",
    "    tokenizer_path=Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer=Whitespace()\n",
    "        trainer=WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d34b23e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (3376763086.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    ds_raw=load_dataset('cous_books', f'{config['lang_src']}.{config['lang_tgt']}', split='train')\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "#loading data\\\n",
    "def get_ds(config):\n",
    "    '''This function loads the data from hugging face making use of load_dataset. It then builds the tokenizer using the loaded dataset. After which datas\n",
    "    t is split into training 90% and validation 10% using torch random_split function'''\n",
    "    ds_raw=load_dataset('cous_books', f'{config['lang_src']}.{config['lang_tgt']}', split='train')\n",
    "\n",
    "    tokenizer_src=build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt=build_tokenizer(config,ds_raw,config['lang_tgt'])\n",
    "\n",
    "    train_ds_size=int(0.9*len(ds_raw))\n",
    "    val_ds_size=len(ds_raw)-train_ds_size\n",
    "    train_ds_raw,val_ds_raw=random.split(ds_raw,[train_ds_size,val_ds_size])\n",
    "\n",
    "\n",
    "    train_ds=BilingualDataset(train_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])\n",
    "    val_ds=BilingualDataset(val_ds_raw,tokenizer_src,tokenizer_tgt, config[['lang_src'],config['lang_tgt'],config['seq_len']])\n",
    "\n",
    "    max_len_src=0\n",
    "    max_len_tgt=0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids=tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids=tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "\n",
    "        max_len_src=max(max_len_src, len(src_ids))\n",
    "        max_len_tgt=max(max_len_tgt,len(tgt_ids))\n",
    "\n",
    "    print(f\" Maximum length of source sentence is {max_len_src}\")\n",
    "    print(f\"Maximum length of target id is {max_len_tgt}\")\n",
    "    #creating dataloaders\n",
    "\n",
    "    train_dataloader=DataLoader(train_ds,batch_size=config['batch_size'],shuffle=True)\n",
    "    val_dataloader=DataLoader(val_ds,batch_size=1,shuffle=False)\n",
    "\n",
    "    return train_dataloader,val_dataloader,tokenizer_src,tokenizer_tgt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cdd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "def get_model(config, vocab_src_len,vocab_tgt_len):\n",
    "    model=build_transformer(vocab_src_len,vocab_tgt_len,config['seq_len'],config['seq_len'],config['d_model'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model,source,source_mask,tokenizer_src,tokenizer_tgt,max_len,device):\n",
    "    sos_ids=tokenizer_src.token_to_id('[SOS]')\n",
    "    eos_ids=tokenizer_src.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output=model.encode(source,source_mask)\n",
    "\n",
    "    decoder_input=torch.empty(1,1).fill_(sos_ids).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1)==max_len:\n",
    "            break\n",
    "        decoder_mask=casual_mask(decoder_input.size(1)).type_as(source).to(device)\n",
    "\n",
    "        out=model.decode(encoder_output,source_mask,decoder_input,decoder_mask)\n",
    "\n",
    "        prob=model.project(out[1,-1])\n",
    "        _,next_word=torch.max(prob,dim=1)\n",
    "\n",
    "        decoder_input=torch.cat([decoder_input, torch.empty(1,1)].type_as(source).fill_(next_word.item().to(device)),dim=1)\n",
    "\n",
    "        if next_word==eos_ids:\n",
    "            break\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference mode\n",
    "def run_validation(model,validation_ds,tokenizer_src,tokenizer_tgt,max_len,device,print_msg,global_state,writer,num_examples):\n",
    "    model.eval()\n",
    "    count=0\n",
    "    source_texts=[]\n",
    "    expected=[]\n",
    "    predicted=[]\n",
    "    console_width=50\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count+=1\n",
    "            encoder_input=batch['encoder_input'].to(device)\n",
    "            encoder_mask=batch['encoder_mask'].to(device)\n",
    "\n",
    "            assert encoder_input.size(0)==1\n",
    "\n",
    "            model_out=greedy_decode(model,encoder_input,encoder_mask,tokenizer_src,tokenizer_tgt,max_len,device)\n",
    "\n",
    "            source_text=batch['src_text'][0]\n",
    "            target_text=batch['tgt_text'][0]\n",
    "            model_out_text=tokenizer_tgt.decode(model_out.detach().cpu().numpy)\n",
    "\n",
    "            source_text.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            print_msg('-' console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f\"TARGET:{target_text}\")\n",
    "            print_msg(f\"Predicted:{model_out_text}\")\n",
    "\n",
    "            if count==num_examples:\n",
    "                break\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594969c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device='cuda ' if torch.cuda.is_available else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    Path(config['model_folder']).mkdir(parents=True,exist_ok=True)\n",
    "    train_dataloader,val_dataloader,tokenizer_src,tokenizer_tgt=get_ds(config)\n",
    "    model=get_model(config, tokenizer_src.get_vocab_size(),tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    writer=SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=config['lr'],eps=1e-9)\n",
    "\n",
    "    initial_epoch,global_step=0,0\n",
    "\n",
    "    if config['preload']:\n",
    "        model_filename=get_weights_file_path(config,config['preload'])\n",
    "        state=torch.load(model_filename)\n",
    "        initial_epoch=state['epoch']+1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step=state['global_step']\n",
    "\n",
    "    loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        \n",
    "        batch_iterator=tqdm(train_dataloader, desc='processing epochs')\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "            model.train()\n",
    "            encoder_input=batch['encoder_input'].to(device)\n",
    "            decoder_input=batch['decoder_input'].to(device)\n",
    "            encoder_mask=batch['encoder_mask'].to(device)\n",
    "            decoder_mask=batch['encoder_mask'].to(device)\n",
    "\n",
    "            encoder_output=model.encode(encoder_input,encoder_mask)\n",
    "            decoder_output=model.decode(encoder_output,encoder_mask,decoder_input,decoder_mask)\n",
    "            proj_output=model.project(decoder_output)\n",
    "\n",
    "            label=batch['label'].to(device)\n",
    "\n",
    "            loss=loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({f\"loss: f{loss.item():.3f}\"})\n",
    "\n",
    "            writer.add_scaler('train loss',loss.item(),global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step+=1\n",
    "\n",
    "        run_validation(model,val_dataloader,tokenizer_src,tokenizer_tgt,config['seq_len'],device,lambda msg :batch_iterator.write(msg),global_step,writer)\n",
    "\n",
    "\n",
    "        model_filename=get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch':epoch,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'global_step':global_step\n",
    "\n",
    "        },model_filename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config=get_config()\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605d71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
