{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980cbce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9849d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first layer in the transformer: the input embeddings\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,d_model:int,vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding=nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)*math.sqrt(self.dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02743c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second layer: positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int, seq_len:int,dropout:float) ->None:\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.seq_len=seq_len\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        #creating matrix shape(seq_len,d_model)\n",
    "        pe=torch.zeros(seq_len,d_model)\n",
    "        #vector od size(seq_len,1)\n",
    "        #applying the positional encoding fromulars(sin,cos)\n",
    "        position=torch.arange(0,seq_len,dtype=torch.float32).unsqueeze(1)\n",
    "        div_term=torch.exp(torch.arange(0,d_model,2).float()*(math.log(10000)/d_model))\n",
    "         #sin for even indeces\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        #ccos for odd indeces\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "         #adding an extra dimension ie(1,seq_len,d_model)\n",
    "        pe=pe.unsqueeze(0)\n",
    "        #registering the tensor to the module buffer\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self,x):\n",
    "        x=x+(self.pe[:, :x.shape[1],:]).requires_grad(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd47323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 3: layer normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,eps:float =10**-6)->None:\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        #multiplicative\n",
    "        self.alpha=nn.Parameter(torch.ones(1))\n",
    "        #additive\n",
    "        self.bias=nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        std=x.std(dim=-1,keepdim=True)\n",
    "        return self.alpha*(x-mean)/ (std+self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537fb202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 5 : the feed forward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model:int,d_ff:int,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.linear_1=nn.Linear(d_model,d_ff)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear_2=nn.Linear(d_ff,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fc291",
   "metadata": {},
   "source": [
    "-multihead attention takes the output of positional encoding and uses it three times \n",
    "as key,query and value as its \n",
    "\n",
    "key,query and value are multiplied by some matrix(weights) to obtain some matrix of same size as the input\n",
    "the new matrices are then each split severally to create several heads\n",
    "\n",
    "attention is then applied to all heads\n",
    "concatination of heads is done then multiply by some weights to obtain the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 6: multihead attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self,d_model:int,h:int,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.h=h\n",
    "        #ensure dmodel is divisle by h\n",
    "        assert d_model % h==0, \"d_model is not divisible by h\"\n",
    "        self.d_k=d_model/h\n",
    "        \n",
    "        #weight matrices\n",
    "        self.w_q=nn.Linear(d_model,d_model)#for query\n",
    "        self.w_k=nn.Linear(d_model,d_model)#for key\n",
    "        self.w_v=nn.Linear(d_model,d_model)#for value\n",
    "         #weight to multiply the heads later after concatination\n",
    "        self.wo=nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "     #calculating attention using the formular( Vaswan et al.,2017)\n",
    "    @staticmethod #creating static method to avoid creating class instance b4 calling it\n",
    "    def attention(query,key,value,mask,dropout:nn.Dropout):\n",
    "        d_k=query.shape[-1]\n",
    "        attention_scores=(query @ key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0, -1e9)\n",
    "        attention_scores=attention_scores.softmax(dim=-1) # (batch,h, seq_len,d_model)\n",
    "        if dropout is not None:\n",
    "            attention_scores=dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "\n",
    "    def forward(self,q,k,v,mask):\n",
    "        #(batch,seq_len,d_model) => (batch,seq_len,d_model)\n",
    "        query=self.w_q(q) \n",
    "        key=self.w_k(k)\n",
    "        value=self.w_v(v)\n",
    "\n",
    "         #(barch,seqlen,d_model)=>(batch,seq_len, h, d_k)=>(batch,h,seqlen,d_k)\n",
    "        query=query.view(query.shape[0], query.shape[1],self.h, self.d_k).transpose(1,2)\n",
    "        key=key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value=value.view(value.shape[0],value.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "\n",
    "        x, self.attention_score = MultiheadAttention.attention(query,key,value,mask,self.dropout)\n",
    "        #(batch,h,seqlen,dk)=>(batch,seqlen,h,d_k)=====>(batch,seqlen,dmodel)\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
    "\n",
    "        return self.w_o(x)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f94661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Connection layer now to do the connection\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout:float) ->None:\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNormalization()\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79012f",
   "metadata": {},
   "source": [
    "since in the paper (Vaswan et al.,2017) the multihead attention layer, add and norm layer, and the feed forward layer are all placed in one block(encoder)\n",
    "then they are Nx number of times, am going to place them now in a block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e1299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block:MultiheadAttention, feed_forward_block:FeedForward,dropout:float)->None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self.self_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "        self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "    def forward(self,x,src_mask):\n",
    "        x=self.residual_connection[0](x,lambda x:self.self_attention_block(x,x,x,src_mask))\n",
    "        x=self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d363a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers:nn.ModuleList)->None:\n",
    "        super().__init__()\n",
    "        self.layer=layers\n",
    "        self.norm=LayerNormalization()\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layer:\n",
    "            x=layer(x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2f3b4",
   "metadata": {},
   "source": [
    "Part 2:   The Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c16413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block:MultiheadAttention, cross_attention_block:MultiheadAttention,feed_forward_block:FeedForward,dropout:float )->None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.cross_attention_block=cross_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "\n",
    "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output,src_mask,tgt_mask):\n",
    "        x=self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,tgt_mask))\n",
    "        x=self.residual_connections[1](x, lambda x: self.cross_attention_block(x,encoder_output,encoder_output,src_mask))\n",
    "        x=self.residual_connections[2](x,self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7461272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.ModuleList):\n",
    "    def __init__(self,layers:nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm=LayerNormalization\n",
    "    def forward(self,x,encoder_output,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,encoder_output,src_mask,tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7552186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,d_model:int, vocab_size:int )->None:\n",
    "        super().__init__()\n",
    "        self.proj=nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,x):\n",
    "        #(batch,seqlen,d_model)---->(batch,seqlen,vocabsize)\n",
    "        return torch.log_softmax(self.proj(x),dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aff22f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer block now!\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder:Encoder, decoder:Decoder,src_embed:InputEmbedding, tgt_embed:InputEmbedding, src_pos:PositionalEncoding, tgt_pos:PositionalEncoding, projection_layer:ProjectionLayer)->None:\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.src_pos=src_pos\n",
    "        self.tgt_pos=tgt_pos\n",
    "        self.projection_layer=projection_layer\n",
    "\n",
    "    def encoder(self,src,src_mask):\n",
    "        src=self.embed(src)\n",
    "        src=self.src_pos(src)\n",
    "        return self.encoder(src,src_mask)\n",
    "    def decoder(self,encoder_output,src_mask,tgt,tgt_mask):\n",
    "        tgt=self.tgt_embed(tgt)\n",
    "        tgt=self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1cb748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size:int,tgt_vocab_size:int,src_seq_len:int,tgt_seq_len:int, d_model:int = 512, N:int=6,h:int=8,dropout:float=0.1, d_ff:int=2048):\n",
    "    src_embed=InputEmbedding(d_model,src_vocab_size)\n",
    "    tgt_embed=InputEmbedding(d_model,tgt_vocab_size)\n",
    "\n",
    "    src_pos=PositionalEncoding(d_model,src_seq_len,dropout)\n",
    "    tgt_pos=PositionalEncoding(d_model,tgt_seq_len,dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block=MultiheadAttention(d_model,dropout)\n",
    "        feed_forward_block=FeedForward(d_model,d_ff,dropout)\n",
    "        encoder_block=EncoderBlock(encoder_self_attention_block,feed_forward_block,dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    decoder_blocks=[]\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block=MultiheadAttention(d_model,h,dropout)\n",
    "        decoder_cross_attention_block=MultiheadAttention(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForward(d_model,d_ff,dropout)\n",
    "        decoder_block=DecoderBlock(decoder_self_attention_block,decoder_cross_attention_block,feed_forward_block,dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    \n",
    "    encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    projection_layer=ProjectionLayer(d_model,tgt_vocab_size)\n",
    "\n",
    "    transformer=Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
